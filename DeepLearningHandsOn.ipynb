{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ディープラーニングハンズオン"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ディープラーニングの基本的な用語や概念を知らない方は、先に「[これから始める人の為のディープラーニング基礎講座](http://images.nvidia.com/content/APAC/events/deep-learning-institute-jp/2017/pdf/nv-murakami.pdf)」に目を通すことを推奨します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 開発環境\n",
    "\n",
    "このハンズオンでは、GPUの搭載されたDLマシンにアクセスし、DIGITSを用いてモデルの学習を行い、最後には、学習済みモデルを用いて推論を行います。\n",
    "\n",
    "前半はDLマシンにブラウザからアクセスできれば、演習が行えます。後半は、PCのDocker上にcaffeが動作する環境があることを想定しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概要\n",
    "ディープラーニングは、ヒトの視覚に近いレベルの認識をコンピュータで実現する技術です。このハンズオンでは、機械学習のワークフローとともに、現実世界の画像認識問題をディープニューラルネットワークを使って解決することにチャレンジします。私たちは、データの前処理、学習、評価、そして精度向上のためのチューニング作業を含むディープラーニングのすべてのワークフローを体験します。また、モデルを学習する際のGPUの恩恵についても確認することになるでしょう。このハンズオンを完了した際には、自身の画像分類データセットを用いて、ディープニューラルネットワークの学習を行うことのできる知識を身に付けています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hello MNIST\n",
    "ディープラーニングでは、多くの中間層をもつ人工ニューラルネットワークの学習を行います。近年、ディープラーニングを用いることにより、コンピュータビジョンや自然言語処理をはじめとした様々な分野でめざましい発展を遂げています。\n",
    "\n",
    "ディープラーニング成功の１つのカギは、畳み込みニューラルネットワーク(Convolutional Neural Network; CNN)です。従来のニューラルネットワークは、前の層と後ろの層のすべてのニューロンが結合した構造(全結合)になっています。CNNでは、局所受容野および重み共有と呼ばれる特別な層間結合を持ちます。生物の脳の視覚野を模倣したCNNは、従来のコンピュータビジョンで行われていたような特徴量の設計を行う必要なく、特徴抽出を可能にしています。\n",
    "\n",
    "ディープラーニングのワークフローを体験するために、DIGITSを使いましょう。DIGITSはGPUを用いたディープラーニングトレーニングシステムです。CNNの開発やテストを簡単に行うことができます。DIGITSは複数のディープラーニングフレームワーク、データ形式をサポートしており、画像分類や物体検出などの学習に対応しています。\n",
    "\n",
    "さらに、DIGITSはワークロードマネージャーの機能も含まれています。ワークロードマネージャーはユーザーに異なるジョブを複数走らせることを可能にします。複数のGPUが存在する場合、ジョブを同時に実行することができます。もしジョブ数が使用可能リソースよりも多い場合でも、ジョブはリソースが使用可能になったタイミングまでキューイングされます。DIGITSのダッシュボードではすべてのアクティブなジョブとその状態を確認することができます。\n",
    "\n",
    "今回のチュートリアルでは、Caffeフレームワークを使用します。CaffeはBerkeley Vision and Learning Center(BVLC)によって作成されたフレームワークで、GPUを用いた高速な処理が可能であり、コードを書くことなくニューラルネットワークの学習が行えるようになっています。モデルの学習はマルチGPUを使って並列化し、高速化することができます。\n",
    "\n",
    "まずは、手書き文字データベース[MNIST](http://yann.lecun.com/exdb/mnist/)を使って、DIGITSの基本操作を体験しましょう。MNISTには、0から9までの手書きの数字の画像が70,000枚含まれています。MNISTはディープラーニングの入門によく使われています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. DIGITSを始めましょう\n",
    "DIGITSを開始すると、以下のようなホーム画面を見ることになります。\n",
    "\n",
    "![](images/digits_home.png)\n",
    "\n",
    "DIGITSを使い新しいデータセットと新しいモデルを作成することが可能です。このホーム画面には、過去に実行した、または実行中のモデルやデータ作成プロセスがすべて表示されます。デフォルトウィンドウパネルはモデルのビューが表示されるようになっています。もしデータセットを見たい場合は、左にある**Datasets**タブを選択してください。右側には**New Dataset**または**New Model**というタブがあります。このタブを選択することで新しいデータセットや新しいモデルを作成することが可能です。クリックすると選択メニューが表示されます。\n",
    "\n",
    "![](images/digits_zoom_new_dataset.png)\n",
    "\n",
    "このチュートリアルでは、**Classification**オプションを使用します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. データセットの作成\n",
    "まずはじめに、データベースをMNISTデータから作成する必要があります。**New Dataset**メニューから**Classification**を選んでください。このとき、あなたはユーザ名を入力する必要があります。このユーザ名は、あなたを識別できるものであれば、なんでもかまいません。\n",
    "\n",
    "**New Dataset**ウィンドウでは、以下のフィールドをセットします。\n",
    "- Image Type : Grayscale\n",
    "- Image Size : 28 x 28\n",
    "- Training Images : /usr/local/images/mnist/train\n",
    "- Select **Separate validation images folder** checkbox\n",
    "    　　※ **Separate test images folder**は使えません\n",
    "- Varidation Images : /usr/local/images/mnist/test\n",
    "- Group Name : [your name]\n",
    "- Dataset Name : MNIST_[your name]\n",
    "\n",
    "\n",
    "![](images/digits_new_classification_dataset.png)\n",
    "\n",
    "フィールドに入力が終わったら**create**ボタンを押してください。\n",
    "\n",
    "次のウィンドウはジョブの進行状況と完了予想時間を表示します。このジョブは数分かかります。\n",
    "\n",
    "終わったら、**Create DB(train)**パネルの下の方にある**Explore**ボタンを探してください。\n",
    "\n",
    "![](images/digits_explore_db.png)\n",
    "\n",
    "ここで、データベースの中のすべての画像を確認することができます。データベースが作成される際、画像の並びはランダムになります。\n",
    "\n",
    "これからDIGITSを用いて、このデータを正しく分類したいと思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1-3. モデルの作成\n",
    "画像のデータベースができました。それを使ってネットワークの学習をしてみましょう。すべての画面の左上には**DIGITS**というアイコンが見えているはずです。それをクリックすると、ホーム画面に戻ることができます。ここでは、**New Model**メニューの**Classification**を選んでモデルの作成に進みます。\n",
    "\n",
    "**New Image Classification Model**ページには、学習のための様々な設定オプションがあります。よく使用するものには以下があります。\n",
    "\n",
    "- **Select Dataset** - データベースから学習に使うものを１つ選択します\n",
    "- **Training Epochs** - 学習エポック回数を選択する。１エポックとはトレーニングデータに対して1反復することです。エポック回数はデータやモデルに依存して異なってきますが、少なくとも5回、100回以上になることもあります\n",
    "- **Snapshot Interval** - モデルの状態をファイルに保存する間隔を何エポックか指定します\n",
    "- **Validation Interval** - バリデーション(評価)データに対してモデルの精度を計算する間隔を何エポックか指定します\n",
    "- **Random Seed** - 乱数生成器で使うシード値を指定します。\n",
    "- **Base Learning Rate** - 重みを更新する際の学習率を指定します。モデルの重みは勾配降下法を用いて計算されます。この値は、各イテレーションでのステップサイズを表します。大きい値では、重みは急速に変更されていきますが、モデルが収束しない場合があります。小さい値では、収束に時間がかかります。\n",
    "\n",
    "DIGITSには、現在３つのネットワークがデフォルトで入っています。LeNetは手書き文字認識のために開発されたCNNです。2012年、ImageNetのコンペティションで優勝したモデルがAlexNetです。AlexNetは従来型の機械学習の代わりにディープラーニングを使用したもので、コンピュータビジョンの世界に革命を起こし、それ以降ImageNetコンペティションのトップはディープニューラルネットワークをベースとしたものになっています。GoogLeNetは、2014年にImageNetコンペティションの画像分類において新たなスタンダードとなったモデルです。\n",
    "\n",
    "DIGITSは２つのフレームワークをサポートしています。Caffeはそのひとつです。Torchはもうひとつのフレームワークであり、画像分類だけでなく音声認識や自然言語処理にも適したものです。\n",
    "\n",
    "私たちのモデルを学習するには、以下のオプションをセットする必要があります。\n",
    "- **MNIST_[your name]**データセットを選択\n",
    "- **Training Epochs**を10にする\n",
    "- **Caffe**フレームワークを選択\n",
    "- **LeNetモ**デルを選択\n",
    "- **Goup Name**を[your name]にする\n",
    "- **Model Name**をLeNet\\_MNIST\\_[your name]する\n",
    "\n",
    "![](images/digits_create_new_model.png)\n",
    "\n",
    "これらのオプションをセットしたら、**Create**ボタンを押してください。モデルの学習が開始します。学習中、モデルの統計データは、ウィンドウ内でどんどん更新されていきます。画面中ほどにあるチャートにはあなたの行っている学習の進行状況が表示されています。\n",
    "\n",
    "![](images/digits_model_loss_accuracy.png)\n",
    "\n",
    "３つの量は、トレーニングLoss、バリデーションLoss、バリデーションAccuracyです。トレーニングLossとバリデーションLossはエポックが進むごとに減少すべきです。Accuracyとは、バリデーションデータを正しく分類するモデルの性能を示す値です。データポイントの上にマウスオーバーすると、正確な値を確認することができます。このケースだと、最後のエポックでのAccuracyは99%です。ネットワークの初期化はランダム性があるため、あなたの結果は少し異なるかもしれません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-4. モデルのテスト\n",
    "それでは、画像を識別する精度をテストしてみましょう。ウィンドウの下の方にスクロールしていくと、1枚の画像または複数の画像をテストすることができます。左側にある**Image Path**に画像パス**/usr/local/images/mnist/test/2/04415.png**というように入力してください。**Show visualizations and statistics**チェックボックスにチェックを入れ、**Classify One**ボタンを押します。数秒後、新しいウィンドウが画像と画像の分類に関する情報とともに表示されます。\n",
    "\n",
    "![](images/digits_classify_one.png)\n",
    "\n",
    "このウィンドウ内では、モデルがどれくらい精度良く認識しているかという情報と、各レイヤーに関する情報が含まれています。99.97%の確率で「２」であるという結果を確認することができます。これは正解ですよね。\n",
    "\n",
    "LeNetネットワークでは、最初の数レイヤーではデータをスケールします。スケールレイヤーとデータレイヤーはオリジナル画像がどのようにスケールされるか、その結果、画像がどのようなものかを示します。conv1レイヤーは5x5のカーネルサイズで20の出力をもっています。conv1のWeightsセクションは5x5のフィルタが20あり、畳み込みレイヤーからの出力画像サイズは24x24です。実際に、conv1 Activationセクションのデータ出力は、**[20 24 24]**とレポートされています。データはCNNを通すと、小さい異なる特徴を獲得していきます。\n",
    "\n",
    "pool1レイヤーは、カーネルサイズが２で、ストライドは２で、MAX Poolingが設定されています。これは、2x2のパッチサイズで2ピクセルスキップしながら計算していき、その最大値を返すということを意味しています。画像のインプットサイズは24x24で、出力画像サイズは12x12です。ネットワークを通す過程で、データがどのように変換されるのかを確認することができます。\n",
    "\n",
    "総学習パラメータは431,080個になります。これは大量のパラメータ数だと感じるかもしれませんが、他のネットワークと比較すると、とても小さい値です。AlexNetやGoogLeNetのようなネットワークは、数千万や数億のパラメータをもっています。中には、10億以上のパラメータを持つものもあります。\n",
    "\n",
    "以上で、DIGITSの基本的な操作を学習しました。次は、より実践的な演習にチャレンジしましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chars74K\n",
    "\n",
    "この章では、手書き文字(英数字)を認識するモデルの作成を通して、データサイエンティストのワークフローを体験します。\n",
    "\n",
    "今、あなたはタブレット上で書かれた英数字(0-9,A-Z,a-z)の認識精度の検証を依頼されたとしましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. データセットの作成\n",
    "\n",
    "まず、モデルを作成するためのデータセットを探します。すると、[Chars74K](http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/)というデータセットの中に、手書き英数字の画像データセットがありました。EnglishHnd.tgzには、1クラスあたり55枚の画像が用意されています。\n",
    "展開すると、\n",
    "```\n",
    "English/Hnd/Sample001\n",
    "English/Hnd/Sample002\n",
    "English/Hnd/Sample003\n",
    "...\n",
    "```\n",
    "以上のようなフォルダに画像ファイルが格納されています。\n",
    "\n",
    "DIGITSで利用するためには、フォルダをクラス名にする必要があります。これらは**/usr/local/images/Chars74K/Hnd/**に用意してあります。\n",
    "```\n",
    "/usr/local/images/Chars74K/Hnd/0\n",
    "...\n",
    "/usr/local/images/Chars74K/Hnd/9\n",
    "/usr/local/images/Chars74K/Hnd/A\n",
    "...\n",
    "/usr/local/images/Chars74K/Hnd/Z\n",
    "/usr/local/images/Chars74K/Hnd/a\n",
    "...\n",
    "/usr/local/images/Chars74K/Hnd/z\n",
    "```\n",
    "以上のように62クラス(0-9,A-Z,a-z)の分類問題となります。\n",
    "\n",
    "それでは、先ほどと同じように、データセットを作成しましょう。\n",
    "今回もLeNetを利用するため、**Image Size**は28x28にしましょう。**Resize Transformation**は文字のアスペクト比を崩さないように**Crop**を選択しましょう。今回は、バリデーションデータが別途用意されていません。データセットのうち19%をバリデーションデータとして利用することにしましょう。(Training:45, Validatoiin:10 /class) \n",
    "\n",
    "![](images/chars74k_create_dataset.png)\n",
    "\n",
    "データセットが作成できたら、中身も確認しておきましょう。\n",
    "\n",
    "![](images/chars74k_data.png)\n",
    "\n",
    "#### (注) データセットはテキストファイルを指定することでも作成できます。\n",
    "**Use Text Files**タブから指定します。\n",
    "\n",
    "train.txtには、\"画像のパス ラベル\"の形式で指定します。ただし、ラベルには0から始める整数値を指定します。\n",
    "```\n",
    "/path/to/image000.png 0\n",
    "/path/to/image001.png 1\n",
    "/path/to/image002.png 2\n",
    "...\n",
    "```\n",
    "\n",
    "labels.txtには、クラス名を順番に記述します。\n",
    "```\n",
    "airplane\n",
    "automobile\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. 最初のモデルの作成\n",
    "\n",
    "まずは、先ほどと同じようにLeNetを試してみましょう。\n",
    "\n",
    "**Select Dataset**で作成したChars74Kのデータセットを選択しましょう。**Training epochs**はデータ数が少ないので、多めに60に、**Subtract Mean**は文字認識なので、**None**を選択することにしましょう。LeNetでどれほどの精度が出るのか試してみます。\n",
    "\n",
    "DIGITSでは、最後の全結合層の出力数をがデータセットのクラス数に自動で設定されます。そのため、今回のデータセットに対しても、LeNetの構成を変更することなく、試すことができます。\n",
    "\n",
    "![](images/chars74k_model.png)\n",
    "\n",
    "学習終了時点のAccuracyは約65%です。これでは、十分な精度とは言えなさそうです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. モデルのカスタマイズ\n",
    "\n",
    "LeNetをベースにモデルを以下のようにカスタマイズしてみましょう。\n",
    "![](images/LeNet_customize.png)\n",
    "\n",
    "**Clone Job**を押して、モデル作成画面を開きます。そして、**LeNet**の右側の**Customize**をクリックして、ネットワークをカスタマイズします。\n",
    "![](images/chars74k_clone.png)\n",
    "\n",
    "\n",
    "まずは、スケールレイヤーを変更しましょう。パラメータ**scale**に0.00390625(=1/256)を指定すると、入力データのピクセル値[0, 255]を[0, 1)にスケーリングします。\n",
    "```\n",
    "layer {\n",
    "  name: \"scale\"\n",
    "...\n",
    "  power_param {\n",
    "    scale: 0.00390625\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "次に、conv1のnum_outputを20から75に、conv2のnum_outputを50から100に変更します。\n",
    "```\n",
    "layer {\n",
    "  name: \"conv1\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"scaled\"\n",
    "  top: \"conv1\"\n",
    "...\n",
    "  convolution_param {\n",
    "    num_output: 75\n",
    "...\n",
    "layer {\n",
    "  name: \"conv2\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"pool1\"\n",
    "  top: \"conv2\"\n",
    "...\n",
    "  convolution_param {\n",
    "    num_output: 100\n",
    "```\n",
    "\n",
    "さらに、畳み込み層conv1とconv2の直後にReLU関数を追加しましょう。\n",
    "```\n",
    "layer {\n",
    "  name: \"reluC1\"\n",
    "  type: \"ReLU\"\n",
    "  bottom: \"conv1\"\n",
    "  top: \"conv1\"\n",
    "}\n",
    "```\n",
    "```\n",
    "layer {\n",
    "  name: \"reluC2\"\n",
    "  type: \"ReLU\"\n",
    "  bottom: \"conv2\"\n",
    "  top: \"conv2\"\n",
    "}\n",
    "```\n",
    "\n",
    "カスタマイズが終わったら、**Visualize**ボタンで可視化して、意図した通りにカスタマイズできたか確認しましょう。\n",
    "![](images/chars74k_visualize.png)\n",
    "\n",
    "確認できたら、**Create**ボタンを押してください。\n",
    "今度はAccuracyが67%になりました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4. 層を深くする\n",
    "\n",
    "次は、モデルをより\"ディープ\"にして、精度向上を図ってみましょう。\n",
    "\n",
    "conv1とconv2の直後に畳み込み層を追加してみましょう。ただし、畳み込み層を通した後に画像サイズが変わらないように、カーネルサイズを5、パディングを2に設定します。また、ReLUも追加します。プーリング層のbottomが変わることにも注意してください。\n",
    "```\n",
    "layer {\n",
    "  name: \"conv1a\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"conv1\"\n",
    "  top: \"conv1a\"\n",
    "  param {\n",
    "    lr_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    lr_mult: 2\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 75\n",
    "    kernel_size: 5\n",
    "    stride: 1\n",
    "    pad: 2\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"reluC1a\"\n",
    "  type: \"ReLU\"\n",
    "  bottom: \"conv1a\"\n",
    "  top: \"conv1a\"\n",
    "}\n",
    "```\n",
    "![](images/chars74k_deep.png)\n",
    "\n",
    "モデルをディープにした分、計算に時間がかかります。\n",
    "\n",
    "学習が終わると、Accuracyは約70%であり、先ほどの結果とあまり変わりません。モデルを複雑にして、精度向上を図ったはずなのに、期待通りの結果にはなりませんでした。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-5. Data Augmentation\n",
    "\n",
    "よりディープなモデルの学習には、その分多くのデータを必要とします。ここでは、データを疑似的に増やすことで、精度向上を図ります。\n",
    "\n",
    "データをよく見ると、文字が左よりに書いてあったり、下の方に書いてあったりします。しかし、文字が書かれている位置は、文字の識別には関係ありません。そこで、32x32の画像から28x28の領域をランダムに切り出すことで、文字が書かれている位置が異なるデータを作り出します。\n",
    "\n",
    "まず、**Datasets**ウィンドウを開き、Chars74K\\_Hnd\\_[your name]を選択し、**Clone Job**をクリックします。**Image Size**を32x32に変更します。**Dataset Name**をChars74K\\_Hnd32\\_[your name]に変更し、**Create**をクリックします。\n",
    "\n",
    "つぎに、先ほどのモデルから**Clone Job**を押し、データセットを32x32のものに変更し、**Data Tranformations**の**Crop Size**を28に設定します。\n",
    "\n",
    "![](images/chars74k_crop.png)\n",
    "\n",
    "今度は、約80%まで精度が向上しました。\n",
    "\n",
    "#### 問\n",
    "一般物体認識では、左右反転によるデータ拡張もよく行われている。それには、Dataレイヤーで以下のように設定する。\n",
    "しかし、今回のタスクでは適していないと考えられる。その理由を考えよ。\n",
    "```\n",
    "layer {\n",
    "  name: \"train-data\"\n",
    "  type: \"Data\"\n",
    "...\n",
    "  transform_param {\n",
    "    mirror: true\n",
    "  }\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-6. 学習済みモデルの利用\n",
    "\n",
    "もし、目的のタスクと似たようなタスクで学習したモデル(学習済みモデル)があれば、それを初期値として利用することで、精度を向上させることができます。\n",
    "\n",
    "Chars74KにはEnglishFntというデータもあり、これはコンピュータのフォントにイタリックやボールドなどを組み合わせた画像(128x128, 1016枚/class)です。今回、われわれの目的は手書き文字を認識させることですが、フォントを認識するタスクはとても良く似たタスクです。\n",
    "そこで、EnglishFntを認識するモデルを作成し、それを学習済みモデルとして利用しましょう。\n",
    "\n",
    "まずは、EnglishFntのデータセットを作成します。データは**/usr/local/images/Chars74K/Fnt**に用意してあります。\n",
    "![](images/chars74k_fnt_dataset.png)\n",
    "\n",
    "つぎに、EnglishFntのデータセットで2-5と同じ構成のモデルを学習します。学習が終わったら、**Make Pretrained Model**をクリックします。\n",
    "![](images/digits_make_pretrained_model.png)\n",
    "\n",
    "今度は、EnglishHndのデータセットで学習します。この際、**Pretrained Networks**から先ほど学習させたPretrained Modelを選びます。\n",
    "また、学習済みモデルを利用する場合には、すでに良い初期値が得られているため、学習係数を下げて学習を始めることがあります。ハイパーパラメータの異なるいくつかのモデルを試したいときには、DIGITSでは、「,」に続けて値を入力することで、複数のモデルを試すことができます。\n",
    "![](images/chars74k_pretrained.png)\n",
    "\n",
    "今回の例では、学習係数が0.01の方が良い結果が得られました。ついに、85%近くまで精度が向上しました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-7. デプロイ\n",
    "\n",
    "ここまでモデルの精度を向上させるために、試行錯誤を繰り返してきました。私たちは、はじめは65%程度しかなかった精度を85%近くにまで向上させることに成功しました。\n",
    "\n",
    "今度は、アプリケーションとして完成させましょう。\n",
    "\n",
    "最終的なモデルを**Download Model**からダウンロードしてきましょう。\n",
    "PCのdata/modelというフォルダを作り、その下に展開しましょう。\n",
    "\n",
    "`$ tar zxvf 20170714-011255-0c1c_epoch_60.0.tar.gz`\n",
    "\n",
    "Windowsでペイントを起動しましょう。画像サイズを28x28にして、そこに文字を書いて、test.pngという名前でdata/の下に保存します。\n",
    "\n",
    "![](images/paint.png)\n",
    "\n",
    "それでは、今書いた文字を先ほど学習させたモデルで推論させてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import caffe\n",
    "import numpy as np\n",
    "\n",
    "IMAGE_FILE = 'test.png'\n",
    "SNAPSHOT = 'model/snapshot_iter_2640.caffemodel'\n",
    "MODEL_FILE = 'model/deploy.prototxt'\n",
    "LABELS = []\n",
    "with open('model/labels.txt') as f:\n",
    "    for label in f:\n",
    "        LABELS.append(label)\n",
    "\n",
    "caffe.set_mode_cpu()\n",
    "net = caffe.Classifier(MODEL_FILE, SNAPSHOT, channel_swap=[0], image_dims=(28, 28))\n",
    "input_image = caffe.io.load_image(IMAGE_FILE, color=False)\n",
    "input_image *= 255\n",
    "prediction = net.predict([input_image], False)\n",
    "print LABELS[prediction[0].argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果はどうでしたか？あなたが書いた通りの結果が出たでしょうか？\n",
    "\n",
    "どの程度正しい結果が返ってくるでしょうか？また、正解率の低い文字はどんな文字でしょうか？\n",
    "\n",
    "いろいろ試してみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "このチュートリアルでは、ディープラーニング入門を行い、データサイエンティストのワークフローを体験しました。\n",
    "ディープラーニングの基礎を学び、実際にCaffeやDIGITSを使ってどのように学習ができるのかを学びました。\n",
    "\n",
    "あなたはすでに、新たなデータセットに対してディープニューラルネットワークの学習を行うことのできる知識を身に付けています。\n",
    "それを確かめるには、[CIFAR](https://www.cs.toronto.edu/~kriz/cifar.html)データセットの分類にチャレンジしてみるとよいでしょう。CIFARは32x32のカラー画像、6万枚からなるデータセットで、犬、猫、トラックなどのクラスを含んでいます。CIFAR10は10クラスから構成されており、CIFAR100は100クラスから構成されています。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
